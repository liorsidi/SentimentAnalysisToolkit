{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Setting\n",
    "## Data, Variables and configuration\n",
    "\n",
    "for the entire project we want to keep same configuration among experiments, the following variables will be used for our diffrent algorithms\n",
    "\n",
    "we will use 5 fold cross validation, along with stratified to each fold with balanced labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from Data import get_raw_data\n",
    "\n",
    "k_folds = 3\n",
    "skf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=1)\n",
    "\n",
    "raw_path = \"Data\\\\sentiment_raw.csv\"\n",
    "\n",
    "labels = {'pos': 1, 'neg': 0}\n",
    "\n",
    "x_raw, y_raw = get_raw_data(raw_path,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Traditional Algorithm\n",
    "\n",
    "use traditional text minning and machine learning and find best sentiment classifier\n",
    "\n",
    "we choose 6 most popular classification algorithm, we used the same baseline feature extraction and selection for all of them, the features are tfidf with standard configuration and are selected the best 50% features using chi^2 estimation\n",
    "Results:\n",
    "\n",
    "the Linear SVM showed the best results\n",
    "\n",
    "## A.1. Find best feature extractor - Tfidf VS Count\n",
    "\n",
    "Tfidf won with 0.894465 vs 0.852758"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#the models we will evaluate\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from Evaluation import evaluate_classifiers\n",
    "\n",
    "classifiers = {\n",
    "    \"Nearest Neighbors\" : KNeighborsClassifier(10),    \n",
    "    \"Decision Tree\" : DecisionTreeClassifier(max_depth=5,min_samples_split=20, random_state=1),\n",
    "    \"Naive Bayes\" : GaussianNB(),\n",
    "    \"AdaBoost\" : AdaBoostClassifier(),\n",
    "    \"Random Forest\": RandomForestClassifier(max_depth=5, n_estimators=100, max_features=1),\n",
    "    \"SGD\" : SGDClassifier(loss = 'log', alpha = 0.00001,penalty = 'l2', n_iter = 50, random_state=1),\n",
    "    \"Linear SVM\" : SVC(kernel='linear', probability=True,random_state=1)\n",
    "    }\n",
    "    \n",
    "#eval tfidt\n",
    "basic_tfidf = TfidfVectorizer()\n",
    "tfidf_pipe = Pipeline([('extractor', basic_tfidf)])\n",
    "eval_tfidf = evaluate_classifiers(x_raw, y_raw,labels,classifiers,skf,tfidf_pipe, False,True,\"eval tfidf vector\")\n",
    "\n",
    "#eval count\n",
    "basic_count = CountVectorizer()\n",
    "count_pipe = Pipeline([('extractor', basic_count)])\n",
    "eval_count = evaluate_classifiers(x_raw, y_raw,labels,classifiers,skf,count_pipe,False, True,\"eval count vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare\n",
    "eval_tfidf = eval_tfidf.sort(columns = ['AUC'], axis =0, ascending  = False)\n",
    "eval_count = eval_count.sort(columns = ['AUC'], axis =0, ascending  = False)\n",
    "\n",
    "tfidf_auc = eval_tfidf[\"AUC\"][0]\n",
    "count_auc = eval_count[\"AUC\"][0]\n",
    "\n",
    "best_extractor = {}\n",
    "if  tfidf_auc > count_auc :\n",
    "    best_extractor[\"vector\"] = basic_tfidf\n",
    "    best_extractor[\"name\"] = \"tfidf\"\n",
    "    best_extractor[\"score\"] = tfidf_auc\n",
    "else:\n",
    "    best_extractor[\"vector\"] = basic_count\n",
    "    best_extractor[\"name\"] = \"count\"\n",
    "    best_extractor[\"score\"] = count_auc\n",
    "\n",
    "best_extractor[\"vector\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = SVC(kernel='linear', probability=True,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimize tfidf for baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize extraction\n",
    "from sklearn.pipeline import Pipeline\n",
    "from Data import Tokenizer, Tokenizer_stemmer\n",
    "from Evaluation import get_best_param_search\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "pipe = Pipeline([\n",
    "        ('extractor', best_extractor[\"vector\"]),\n",
    "        ('clf', best_classifier),\n",
    "    ])\n",
    "\n",
    "params  = {\n",
    "    'extractor__tokenizer': (Tokenizer(), Tokenizer_stemmer()),\n",
    "    'extractor__ngram_range': ((1, 1), (1, 2)), # unigrams or bigrams\n",
    "    'extractor__lowercase' : (True, False),\n",
    "    'extractor__stop_words' : (None, \"english\"),\n",
    "    }\n",
    "\n",
    "\n",
    "tfidf_extract_search = get_best_param_search(x_raw,y_raw,pipe,params,\"extract\")\n",
    "\n",
    "\n",
    "\n",
    "if (tfidf_extract_search.best_score_ > best_extractor[\"score\"] ):\n",
    "    best_extractor[\"vector\"] = tfidf_extract_search.best_estimator_.named_steps['extractor']\n",
    "    best_extractor[\"name\"] = \"tfidf_extract\"\n",
    "    best_extractor[\"score\"] = tfidf_extract_search.best_score_\n",
    "\n",
    "# optimize extraction\n",
    "\n",
    "np.random.seed(0)\n",
    "pipe = Pipeline([\n",
    "        ('extractor', best_extractor[\"vector\"]),\n",
    "        ('clf', best_classifier),\n",
    "    ])\n",
    "\n",
    "if (tfidf_extract_search_ngram.best_score_ > best_extractor[\"score\"]):\n",
    "    best_extractor[\"vector\"] = tfidf_extract_search_ngram.best_estimator_.named_steps['extractor']\n",
    "    best_extractor[\"name\"] = \"tfidf_ngram_extract\"\n",
    "    best_extractor[\"score\"] = tfidf_extract_search_ngram.best_score_\n",
    "\n",
    "\n",
    "\n",
    "params  = {\n",
    "    'extractor__tokenizer': (Tokenizer(), Tokenizer_stemmer()),\n",
    "    'extractor__ngram_range': ((1, 1), (1, 2),(1,3)), # unigrams or bigrams\n",
    "    'extractor__lowercase' : (True, False),\n",
    "    'extractor__stop_words' : (None, \"english\"),\n",
    "    }\n",
    "\n",
    "\n",
    "tfidf_extract_search_ngram = get_best_param_search(x_raw,y_raw,pipe,params,\"extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize tdif words selection\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from Data import union_param\n",
    "from Evaluation import evaluate_vocabulary\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('extractor',TfidfVectorizer()) ,\n",
    "        ('clf', best_classifier),\n",
    "    ])\n",
    "\n",
    "params  = {\n",
    "    'extractor__max_df': (0.5, 0.7, 1.0),\n",
    "    'extractor__min_df': (0.005, 0.01,0.5, 1),\n",
    "    'extractor__max_features': (500, 1500,2000,None),\n",
    "    }\n",
    "\n",
    "params = union_param(params,tfidf_extract_search.best_params_)\n",
    "tfidf_extract_select_search = get_best_param_search(x_raw,y_raw,pipe,params,\"tfidf_extract_select\")\n",
    "\n",
    "\n",
    "\n",
    "if (tfidf_extract_select_search.best_score_ > best_extractor[\"score\"]):\n",
    "    best_extractor[\"vector\"] = tfidf_extract_select_search.best_estimator_.named_steps['extractor']\n",
    "    best_extractor[\"name\"] = best_extractor[\"name\"] + \"_extract_idfs\"\n",
    "    best_extractor[\"score\"] = tfidf_extract_search.best_score_\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "pipe = Pipeline([\n",
    "        ('extractor', best_extractor[\"vector\"]),\n",
    "        ('clf', best_classifier),\n",
    "    ])\n",
    "\n",
    "params  = {\n",
    "    'extractor__norm': (None, 'l2'),\n",
    "    'extractor__sublinear_tf': (True,False),\n",
    "    }\n",
    "\n",
    "#params = union_param(params,tfidf_extract_select_search.best_params_)\n",
    "tfidf_extract_select_transform_search = get_best_param_search(x_raw,y_raw,pipe,params,\"tfidf_extract_select_transform\")\n",
    "\n",
    "if (tfidf_extract_select_transform_search.best_score_ > best_extractor[\"score\"] ):\n",
    "    best_extractor[\"vector\"] = tfidf_extract_select_transform_search.best_estimator_.named_steps['extractor']\n",
    "    best_extractor[\"name\"] = best_extractor[\"name\"] + \"_transform\"\n",
    "    best_extractor[\"score\"] = tfidf_extract_search.best_score_\n",
    "\n",
    "\n",
    "emoticons = []\n",
    "with open(\"Data\\\\emoticons.txt\") as f:\n",
    "    for line in f:\n",
    "        emoticons.append(line.split()[0])\n",
    "        \n",
    "print(len(best_extractor[\"vector\"].vocabulary_))\n",
    "eval_vec1, tf_emotic_vec_enrich = evaluate_vocabulary(x_raw,y_raw, labels, best_classifiers,skf,\n",
    "                                    False, emoticons, best_extractor[\"vector\"],\"tf_emotic_enrich\",False)\n",
    "    \n",
    "eval_vec2, tf_emotic_vec_Replace = evaluate_vocabulary(x_raw,y_raw, labels, best_classifiers,skf,\n",
    "                                    False, emoticons, best_extractor[\"vector\"],\n",
    "                                    \"tf_emotic_enrich\",True)\n",
    "\n",
    "enrich_auc = eval_vec1[\"AUC\"][0]\n",
    "replace_auc = eval_vec2[\"AUC\"][0]\n",
    "\n",
    "if  enrich_auc > best_extractor[\"score\"] :\n",
    "    best_extractor[\"vector\"] = tf_emotic_vec_enrich\n",
    "    best_extractor[\"name\"] = best_extractor[\"name\"] + \"_enrich\"\n",
    "    best_extractor[\"score\"] = enrich_auc\n",
    "\n",
    "if  replace_auc > best_extractor[\"score\"] :\n",
    "    best_extractor[\"vector\"] = tf_emotic_vec_Replace\n",
    "    best_extractor[\"name\"] = best_extractor[\"name\"] + \"_replace\"\n",
    "    best_extractor[\"score\"] = replace_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## A.2 evaluate feature selection methods Chi^2 VS PMI\n",
    "\n",
    "chi wom with 0.891369 vs 0.874022 but worse the none\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize feature selection\n",
    "pipe = Pipeline([\n",
    "        ('extractor', best_extractor[\"vector\"]),\n",
    "        ('selector', SelectPercentile()),\n",
    "        ('clf', best_classifier),\n",
    "    ])\n",
    "params = {\n",
    "    'selector__score_func': (chi2,f_classif, mutual_info_classif),\n",
    "    'selector__percentile': (10, 25,50,75,100),\n",
    "    }\n",
    "\n",
    "params = union_param(params,tfidf_extract_select_search.best_params_)\n",
    "\n",
    "tfidf_extract_select_transform_search_fs = get_best_param_search(x_raw,y_raw,pipe,params,\"tfidf_extract_select_transform_fs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if (tfidf_extract_select_transform_search_fs.best_score_ > best_extractor[\"score\"] ):\n",
    "    best_extractor[\"vector\"] = tfidf_extract_select_transform_search_fs.best_estimator_.named_steps['extractor']\n",
    "    best_extractor[\"selector\"] = tfidf_extract_select_transform_search_fs.best_estimator_.named_steps['selector']\n",
    "    best_extractor[\"name\"] = best_extractor[\"name\"] + \"_fs\"\n",
    "    best_extractor[\"score\"] = tfidf_extract_select_transform_search_fs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 compare topic analysis algorithms - LDA VS LSA\n",
    "use topic analysis as feature selection using vocabulary\n",
    "\n",
    "LDA wom with 0.892090 vs 0.874022 but worse the none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tain LDA on best tf piipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from Evaluation import print_topics\n",
    "\n",
    "n_topics =20\n",
    "\n",
    "select_LDA = LatentDirichletAllocation(n_topics=n_topics,\n",
    "                                          max_iter=100,\n",
    "                                          learning_method= 'batch',\n",
    "                                          random_state=0)       \n",
    "\n",
    "pipeline_lda = Pipeline([\n",
    "        ('extractor', best_extractor[\"vector\"]),\n",
    "        ('clf', select_LDA),\n",
    "    ])\n",
    "\n",
    "x_transformed = pipeline_lda.fit_transform(x_raw,y_raw )\n",
    "joblib.dump(pipeline_lda, pickl_path + 'pipeline_lda.pkl')\n",
    "\n",
    "\n",
    "\n",
    "print_topics(n_topics, pipeline_lda,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tain LDA on best tf piipeline\n",
    "from Evaluation import get_top_words\n",
    "\n",
    "n_topics =20\n",
    "select_LDA = LatentDirichletAllocation(n_topics=n_topics,\n",
    "                                          max_iter=100,\n",
    "                                          learning_method= 'batch',\n",
    "                                          random_state=0)       \n",
    "\n",
    "pipeline_lda = Pipeline([\n",
    "        ('extractor', CountVectorizer(stop_words = \"english\", min_df = 20)),\n",
    "        ('clf', select_LDA),\n",
    "    ])\n",
    "\n",
    "x_transformed = pipeline_lda.fit_transform(x_raw,y_raw )\n",
    "joblib.dump(pipeline_lda, pickl_path + 'pipeline_lda.pkl')\n",
    "\n",
    "\n",
    "\n",
    "print_topics(n_topics, pipeline_lda,False)\n",
    "\n",
    "\n",
    "\n",
    "LDA_top_words = get_top_words(pipeline_lda, 1000,False,1)\n",
    "LDA_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI topic analysis\n",
    "n_components = 20\n",
    "select_LSI = TruncatedSVD(n_components=n_components, n_iter=7, random_state=42)\n",
    "tfidf_lsi_pipe = Pipeline([('extractor', best_extractor[\"vector\"]),\n",
    "                            ('clf', select_LSI)\n",
    "                               ])\n",
    "x_transformed = tfidf_lsi_pipe.fit_transform(x_raw,y_raw)\n",
    "joblib.dump(tfidf_lsi_pipe, pickl_path + 'tfidf_lsi_pipe.pkl')\n",
    "\n",
    "print_topics(n_components, tfidf_lsi_pipe,False)\n",
    "\n",
    "LSI_top_words = get_top_words(tfidf_lsi_pipe, 1000,False,13)\n",
    "LSI_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize tdif selection\n",
    "\n",
    "lsi_vec = evaluate_vocabulary(x_raw,y_raw, labels, best_classifiers,skf,False,LSI_top_words,\n",
    "                                  tfidf_extract_select_transform_search.best_estimator_.named_steps['extractor'],\n",
    "                                  \"tf_emotic_replace\",\n",
    "                                  True)\n",
    "\n",
    "lda_vec = evaluate_vocabulary(x_raw,y_raw, labels, best_classifiers,skf,False,LDA_top_words,\n",
    "                                  tfidf_extract_select_transform_search.best_estimator_.named_steps['extractor'],\n",
    "                                  \"tf_emotic_replace\",\n",
    "                                  True)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.DataFrame(tfidf_extract_select_transform_search_fs.cv_results_ )\n",
    "df\n",
    "mplt = df.plot(x = \"param_selector__score_func\" , y = \"mean_test_score\" )\n",
    "mplt.set_xlabel(\"func\")\n",
    "mplt.set_ylabel(\"score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Advanced Algorithm\n",
    "in this section we will try to improve the feature selection using the combination of LDA and Word2Vec\n",
    "general explenation:\n",
    "Word2Vec feature selection we will use word2vec algorithms to find words that are sentimential relevant to the label.\n",
    "LDA with word2vec selection we will run LDA on the selected word and try to find topics that relevant to a sentiment label.\n",
    "classification with word2vec selection we will train a LDA classifier and the optimized model and evaluate it\n",
    "\n",
    "## 3.1 use Word2Vec\n",
    "we used word2vec model to search for relevant sentiment words and use them as features:\n",
    "1. we used 2 w2v models:\n",
    "1.1 sentiment model trained on the given dataset\n",
    "1.2 google model on wikipedia\n",
    "2.\"semi - unsupervised\" word seach: in each step we collect the words into one vocabulary that will be used for feature selection\n",
    "2.1 pos and neg lists: from the web\n",
    "2.2 thershold for naive select: used for picking similar words to 'good' and 'bad\n",
    "2.3 best words / naive select: use threshold to find the most similar words to the words 'good' and 'bad'\n",
    "2. enrich words / advance:\n",
    "2.4.1 Tiear 2: for top most similar words also claculate most similar words (a tier 2)\n",
    "2.4.2 Negative similarity: because the labels has negative-positive relation we also used the positive - negative similarityto to find a similar word to the oposit label!\n",
    "3. \"supervised\" word seach: for the k most informetial words we calculate 10 \"best words\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v sentiment model\n",
    "#trained on the given dataset\n",
    "import gensim\n",
    "sentences = []\n",
    "with open('Data\\\\train_unsup.txt') as f:\n",
    "    for line in f:\n",
    "        sentences.append(line.split())\n",
    "model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=5, workers=4)\n",
    "model.save('Models\\\\sentimentModel')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos and neg lists\n",
    "\n",
    "#pos and neg lists: we used positive and negative word lists form: http://ptrckprry.com/course/ssd/\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from Data import extract_calc_words_from_file\n",
    "\n",
    "sentiment2vec  = gensim.models.Word2Vec.load('Models\\\\sentimentModel')\n",
    "neg_words = extract_calc_words_from_file('Data\\\\negative_words.txt',\n",
    "                                       sentiment2vec,\n",
    "                                       ('good','bad')\n",
    "                                       )\n",
    "pos_words = extract_calc_words_from_file('Data\\\\positive_words.txt',\n",
    "                                       sentiment2vec,\n",
    "                                       ('good','bad')\n",
    "                                     )\n",
    "all_words = {} \n",
    "all_words.update(pos_words)\n",
    "all_words.update(neg_words)\n",
    "\n",
    "\n",
    "neg_df = pd.DataFrame(neg_words).transpose()\n",
    "print('Negative words statistics')\n",
    "neg_stats = neg_df.describe()\n",
    "print(neg_stats)\n",
    "\n",
    "pos_df = pd.DataFrame(pos_words).transpose()\n",
    "print('Positive words statistics')\n",
    "pos_stats = pos_df.describe()\n",
    "print(pos_stats)\n",
    "\n",
    "print (\"all words collected from docs = \" + str(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "find similarity threshold\n",
    "\n",
    "finiding thershold for picking similar ward for 'good' and 'bad'\n",
    "for each list we calculate the distance from the words 'good' and 'bad'\n",
    "for each list we calculate the mean and std distance from words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_sim_threshold = neg_stats['bad_sim']['mean'] + neg_stats['bad_sim']['std']\n",
    "pos_sim_threshold = pos_stats['good_sim']['mean'] + pos_stats['good_sim']['std']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words 1 - find Best Words¶\n",
    "\n",
    "best words: we used the mean + std distance as a threshold to find the most similar words to the words 'good' and 'bad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Evaluation import find_best_words\n",
    "\n",
    "words_for_sim = ('good','bad')\n",
    "pos_best_words = {}\n",
    "neg_best_words = {}\n",
    "\n",
    "most_sim = sentiment2vec.most_similar('good', topn = 750)\n",
    "pos_best_words.update(find_best_words(sentiment2vec,most_sim,'good',pos_sim_threshold,words_for_sim,all_words))\n",
    "all_words.update(pos_best_words)\n",
    "\n",
    "most_sim = sentiment2vec.most_similar('bad', topn = 750)\n",
    "neg_best_words.update(find_best_words(sentiment2vec,most_sim,'bad',neg_sim_threshold,words_for_sim,all_words))\n",
    "all_words.update(neg_best_words)\n",
    "\n",
    "print (\"all words after simple w2v = \" + str(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words 2 - enrich word\n",
    "\n",
    "enrich words1: for top most similar words with similarity higher than mean + 2*std we also claculate most similar words enrich words2: because the labels has negative-positive relation we also used the positive - negative similarityto to find a similar word to the oposit label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import enrich_words\n",
    "\n",
    "pos_enrich_words = enrich_words(pos_best_words,sentiment2vec,500,'good','bad',pos_sim_threshold*1.7,('good','bad'),all_words)\n",
    "all_words.update(pos_enrich_words)\n",
    "neg_enrich_words = enrich_words(neg_best_words,sentiment2vec,500,'bad','good',neg_sim_threshold*1.7,('good','bad'),all_words)\n",
    "all_words.update(neg_best_words)\n",
    "\n",
    "print (\"all words after smart w2v = \" + str(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words 3 - Supervised - best for k top words¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "k_words = 100\n",
    "n_best_words = 50\n",
    "\n",
    "pipeline_select_k = Pipeline([\n",
    "        ('extractor', CountVectorizer(stop_words = \"english\", min_df = 20)),\n",
    "        ('selector', SelectKBest(score_func = mutual_info_classif, k = k_words )),\n",
    "         ])\n",
    "\n",
    "opimizer_maniulate = pipeline_select_k.fit_transform(x_raw, y_raw)\n",
    "\n",
    "feature_names = pipeline_select_k.named_steps['extractor'].get_feature_names()\n",
    "support = pipeline_select_k.named_steps['selector'].get_support()\n",
    "feature_names = np.array(feature_names)[support]\n",
    "\n",
    "\n",
    "supervised = {}\n",
    "for select_word in feature_names:\n",
    "    if select_word in sentiment2vec:\n",
    "        top_words = sentiment2vec.most_similar(select_word, topn = n_best_words)\n",
    "        for w , v in top_words:\n",
    "            if w in sentiment2vec:\n",
    "                supervised[w] = {}\n",
    "                supervised[w]['good_sim'] = sentiment2vec.similarity('good',w) \n",
    "                supervised[w]['bad_sim'] = sentiment2vec.similarity('bad',w)\n",
    "\n",
    "\n",
    "all_words.update(supervised)\n",
    "\n",
    "print (\"all words after supervised w2v = \" + str(len(all_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## words 4 - LDA with W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.decomposition\n",
    "\n",
    "n_topics = 20\n",
    "\n",
    "select_LDA = sklearn.decomposition.LatentDirichletAllocation(n_topics=n_topics,\n",
    "                                                             max_iter=100,\n",
    "                                                             learning_method= 'batch',\n",
    "                                                             random_state=0)\n",
    "\n",
    "tfidf_lda_pipe = Pipeline([('extractor', CountVectorizer(stop_words = \"english\", min_df = 20, vocabulary = all_words.keys())),\n",
    "                           ('clf', select_LDA)\n",
    "                               ])\n",
    "\n",
    "\n",
    "x_transformed = tfidf_lda_pipe.fit_transform(x_raw,y_raw)\n",
    "\n",
    "\n",
    "print_topics(10, tfidf_lda_pipe,False)\n",
    "\n",
    "\n",
    "\n",
    "LDA_top_words = get_top_words(tfidf_lda_pipe, 1000,False,3)\n",
    "LDA_top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classification with W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(supervised)\n",
    "\n",
    "# optimize tdif selection\n",
    "\n",
    "vocabs = {\n",
    "    \"all\" : all_words.keys(),\n",
    "    \"supervised\" : supervised.keys(), \n",
    "    \"negative\" : neg_words.keys(),\n",
    "     \"positive\" : pos_words.keys(),\n",
    "    \"best positive\" : pos_best_words.keys(),\n",
    "     \"best negative\" : neg_best_words.keys(),\n",
    "     \"enrich positive\" : pos_enrich_words.keys(),\n",
    "     \"enrich negative\" : neg_enrich_words.keys() ,\n",
    "}\n",
    "\n",
    "replace = False\n",
    "evals_w2v = {}\n",
    "len_w2v= {}\n",
    "for name, vocab in vocabs.iteritems():\n",
    "    eval_w2v, _ = evaluate_vocabulary(x_raw,y_raw, labels, classifiers,skf,False,vocab,best_extractor[\"vector\"],\"best_w2v_\"+name,True)\n",
    "    evals_w2v[name] =  eval_w2v[\"AUC\"][0]\n",
    "    len_w2v[name] =  len(vocab)\n",
    "\n",
    "    \n",
    "pd.DataFrame(evals_w2v,index=[0]).transpose()\n",
    "\n",
    "\n",
    "# optimize feature selection\n",
    "\n",
    "select_k_eval={}\n",
    "for name, vocab in vocabs.iteritems():\n",
    "    print(\"k compare to \" + name + \"size: \")\n",
    "    print(len(vocab))\n",
    "    pipe = Pipeline([\n",
    "            ('extractor', best_extractor[\"vector\"]),\n",
    "            ('selector', SelectKBest(score_func = chi2, k = len(vocab))),\n",
    "        ])\n",
    "    eval_select = evaluate_classifiers(x_raw, y_raw,labels,classifiers,skf,pipe,False, True,\"eval k select k = \" + str(len(vocab)))\n",
    "    select_k_eval[len(vocab)] = eval_select[\"AUC\"][0]\n",
    "\n",
    "for i  in [1.5,2,2.5,3,3.5,4,4.5,5]:\n",
    "    print(\"k compare to all size: \")\n",
    "    select = len(all_words)*i\n",
    "    print(select)\n",
    "    pipe = Pipeline([\n",
    "            ('extractor', best_extractor[\"vector\"]),\n",
    "            ('selector', SelectKBest(score_func = chi2, k = select)),\n",
    "        ])\n",
    "    eval_select = evaluate_classifiers(x_raw, y_raw,labels,classifiers,skf,pipe,False, True,\"eval k select k = \" + str(select))\n",
    "    select_k_eval[select] =  eval_select[\"AUC\"][0]\n",
    "    \n",
    "    \n",
    "\n",
    "pd.DataFrame(select_k_eval,index=[0]).transpose()\n",
    "\n",
    "count_evals_w2v = {}\n",
    "for name, vocab in vocabs.iteritems():\n",
    "    print name\n",
    "    c_eval_w2v, x_ = evaluate_vocabulary(x_raw,y_raw, labels,\n",
    "                        classifiers,\n",
    "                        skf,False,vocab,\n",
    "                        basic_count,\n",
    "                        \"count_w2v_\"+name,True)\n",
    "    count_evals_w2v[name] =  c_eval_w2v[\"AUC\"][0]\n",
    "\n",
    "    \n",
    "pd.DataFrame(count_evals_w2v,index=[0]).transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}